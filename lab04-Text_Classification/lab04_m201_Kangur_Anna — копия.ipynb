{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 04. Text Classification\n",
    "\n",
    "\n",
    "This lab is devoted to text classification tasks.\n",
    "- **Part 1 [8 points]** is about very common NLP problem - sentiment analysis.\n",
    "- **Part 2 [7 points]** include tasks on POS tagging and WordEmbeddings.\n",
    "\n",
    "\n",
    "#### Evaluation\n",
    "\n",
    "Each task has its value, **15 points** in total. If you use some open-source code please make sure to include the url.\n",
    "\n",
    "#### How to submit\n",
    "\n",
    "- Name your file according to this convention: `lab04_GroupNo_Surname_Name.ipynb`. If you don't have group number, put `nan` instead.\n",
    "- Attach it to an **email** with **topic** `lab04_GroupNo_Surname_Name.ipynb`\n",
    "- Send it to `cosmic.research.ml@yandex.ru`\n",
    "\n",
    "\n",
    "Data can be dowloaded from: https://disk.yandex.ru/d/ixeu6m2KBG80ig\n",
    "\n",
    "The deadline is **2021-11-17 23:00:00 +03:00**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Bag of Words vs. Bag of Popcorn [8 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This task is based on [Bag of Words Meets Bags of Popcorn](https://www.kaggle.com/c/word2vec-nlp-tutorial/data) competition. The goal is to label film reviews as positive or negative. \n",
    "\n",
    "Reviews may look like this:\n",
    "\n",
    "```\n",
    "I dont know why people think this is such a bad movie. Its got a pretty good plot, some good action, and the change of location for Harry does not hurt either. Sure some of its offensive and gratuitous but this is not the only movie like that. Eastwood is in good form as Dirty Harry, and I liked Pat Hingle in this movie as the small town cop. If you liked DIRTY HARRY, then you should see this one, its a lot better than THE DEAD POOL. 4/5\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK (Natural Language Toolkit)  — пакет библиотек и программ для символьной и статистической обработки естественного языка, написанных на языке программирования Python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5814_8</td>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2381_9</td>\n",
       "      <td>1</td>\n",
       "      <td>\\The Classic War of the Worlds\\\" by Timothy Hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7759_3</td>\n",
       "      <td>0</td>\n",
       "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  sentiment                                             review\n",
       "0  5814_8          1  With all this stuff going down at the moment w...\n",
       "1  2381_9          1  \\The Classic War of the Worlds\\\" by Timothy Hi...\n",
       "2  7759_3          0  The film starts with a manager (Nicholas Bell)..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = pd.read_csv(\"reviews.tsv\", sep=\"\\t\")\n",
    "reviews.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = reviews[\"review\"]\n",
    "y = reviews[\"sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=5000, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20000,), (5000,), (20000,), (5000,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time to extract features\n",
    "\n",
    "In this part of the assignment we will apply several methods of feature extraction and comapre them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.1 [0.5 point] - Simple BOW (Bag-of-Words)** \n",
    "\n",
    "In this task we will build a simple bow representation - without any preprocessing. \n",
    "\n",
    "For this purpose we will use [*CountVectorizer*](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer) - a method that transforms text dataset into a [sparse matrix](https://docs.scipy.org/doc/scipy/reference/sparse.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CountVectorizer - сonvert a collection of text documents to a matrix of token counts.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import CountVectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try each of these approaches:\n",
    "- fit vectorizer on X_train, apply to X_train, X_test\n",
    "- fit vectorizer on X_train, apply to X_train; fit on X_test, apply to X_test\n",
    "- fit vectorizer on X, apply to X_train, X_test\n",
    "\n",
    "Report output matrix sizes in each case. \n",
    "- What is the difference? \n",
    "- Which of these approaches is the most fair and correct?\n",
    "\n",
    "Use the most fair and correct one to get `X_train_0` and `X_test_0` - they will be needed for further tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 2],\n",
       "       [1, 1],\n",
       "       [0, 1],\n",
       "       [1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "tmp = vectorizer.fit_transform(['Hi, hi', \n",
    "                                'Hello, hi',\n",
    "                                'Hi',\n",
    "                                'hello'])\n",
    "tmp.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "#from sklearn.metrics import mean_squared_error\n",
    "#from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. fit vectorizer on X_train, apply to X_train, X_test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 68482) (5000, 68482)\n"
     ]
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "#X_0 = count_vectorizer.fit_transform(X.to_list())\n",
    "#X_train_0, X_test_0, y_train_0, y_test_0 = train_test_split(X_0, y, test_size=5000, random_state=42, stratify=y)\n",
    "#print(X_train_0.shape, X_test_0.shape)\n",
    "\n",
    "X_train_0 = count_vectorizer.fit_transform(X_train.to_list())\n",
    "X_test_0  = count_vectorizer.transform(X_test.to_list())\n",
    "print(X_train_0.shape, X_test_0.shape)\n",
    "\n",
    "#cl1 = RandomForestClassifier()\n",
    "#start = time()\n",
    "#cl1.fit(X_train_0, y_train)\n",
    "#end = time()\n",
    "#print('lead time: {:.3f} seconds'.format(end - start))\n",
    "#cl1.fit(X_train_0.toarray(), y_train)\n",
    "#ValueError: array is too big; `arr.size * arr.dtype.itemsize` is larger than the maximum possible size.\n",
    "\n",
    "#cl1_score_train = cl1.score(X_train_0,y_train)\n",
    "#cl1_score_test  = cl1.score(X_test_0,y_test)\n",
    "\n",
    "#pred_train = cl1.predict(X_train_0)\n",
    "#cl1_mse_train   = mean_squared_error(pred_train, y_train)\n",
    "#pred_test = cl1.predict(X_test_0)\n",
    "#cl1_mse_test  = mean_squared_error(pred_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. fit vectorizer on X_train, apply to X_train; fit on X_test, apply to X_test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 68482) (5000, 38591)\n"
     ]
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "X_train_0 = count_vectorizer.fit_transform(X_train.to_list())\n",
    "X_test_0  = count_vectorizer.fit_transform(X_test.to_list())\n",
    "print(X_train_0.shape, X_test_0.shape)\n",
    "\n",
    "#cl2 = RandomForestClassifier()\n",
    "#start = time()\n",
    "#cl2.fit(X_train_0, y_train)\n",
    "#end = time()\n",
    "#print('lead time: {:.3f} seconds'.format(end - start))\n",
    "\n",
    "#cl2_score_train = cl2.score(X_train_0,y_train)\n",
    "#cl2_score_test  = cl2.score(X_test_0,y_test)\n",
    "\n",
    "#pred_train = cl2.predict(X_train_0)\n",
    "#cl2_mse_train   = mean_squared_error(pred_train, y_train)\n",
    "#pred_test = cl2.predict(X_test_0)\n",
    "#cl2_mse_test  = mean_squared_error(pred_test, y_test)\n",
    "\n",
    "#ValueError: Number of features of the model must match the input. Model n_features is 68482 and input n_features is 38591 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. fit vectorizer on X, apply to X_train, X_test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 74849) (5000, 74849)\n"
     ]
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "X_0 = count_vectorizer.fit_transform(X.to_list())\n",
    "X_train_0 = count_vectorizer.transform(X_train.to_list())\n",
    "X_test_0  = count_vectorizer.transform(X_test.to_list())\n",
    "print(X_train_0.shape, X_test_0.shape)\n",
    "\n",
    "#cl3 = RandomForestClassifier()\n",
    "#start = time()\n",
    "#cl3.fit(X_train_0, y_train)\n",
    "#end = time()\n",
    "#print('lead time: {:.3f} seconds'.format(end - start))\n",
    "\n",
    "#cl3_score_train = cl3.score(X_train_0,y_train)\n",
    "#cl3_score_test  = cl3.score(X_test_0,y_test)\n",
    "\n",
    "#pred_train = cl3.predict(X_train_0)\n",
    "#cl3_mse_train   = mean_squared_error(pred_train, y_train)\n",
    "#pred_test = cl3.predict(X_test_0)\n",
    "#cl3_mse_test  = mean_squared_error(pred_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#d = {'score train':  [cl1_score_train,cl2_score_train, cl3_score_train] , \n",
    "#     'score test':   [cl1_score_test, cl2_score_test,  cl3_score_test],\n",
    "#     'mse train':    [cl1_mse_train,  cl2_mse_train,   cl3_mse_train],\n",
    "#     'mse test':     [cl1_mse_test,   cl2_mse_test,    cl3_mse_test]\n",
    "#    }\n",
    "#pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если честно, не особо понимаю, как происходит обучение с Sparse matrices. \n",
    "Она преобразовывается в array как-то динамически? \n",
    "Просто когда попыталась преобразовать, то Питон ругается, мол слишком большой массив.\n",
    "\n",
    "Если динамически преобразование не происходит, боюсь выше фигня написана)\n",
    "\n",
    "Особенно во вротом случае. Матрицы X_train_0 and X_test_0 как минимум должны быть одинаковой размерности."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.2 [0.5 point] - S___se matrices**\n",
    "\n",
    "What is the data type of `X_train_0` and `X_test_0`? What are those?\n",
    "\n",
    "What differs them from usual np.arrays? Name several types how those special matrices are stored and what they are good for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**fit_transform()** and **transform()** returns X: sparse matrix of shape (n_samples, n_features), document-term matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html\n",
    "\n",
    "Compressed Sparse Row matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<20000x74849 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 2760558 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_0.toarray()\n",
    "#ValueError: array is too big; `arr.size * arr.dtype.itemsize` is larger than the maximum possible size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 2],\n",
       "       [0, 0, 3],\n",
       "       [4, 5, 6]], dtype=int32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "row = np.array([0, 0, 1, 2, 2, 2])\n",
    "col = np.array([0, 2, 2, 0, 1, 2])\n",
    "data = np.array([1, 2, 3, 4, 5, 6])\n",
    "csr_matrix((data, (row, col))).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know the places with the location of the data, the rest are zeros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sparse matrix classes:** https://docs.scipy.org/doc/scipy/reference/sparse.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разряженные матрицы содержат много информации, но занимают значительно меньше памяти."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.3 [1 points] - Training**\n",
    "\n",
    "Train LogisticRegression and Random forest on this data representations.\n",
    "- Compare training time \n",
    "- Compare Accuracy, precision, recall \n",
    "- Plot ROC Curve and calculate ROC AUC (don't forget to predict_proba) \n",
    "- Plot Precision-Recall curve and calculate f1-score (for example, with `plt.subplots(nrows=1, ncols=2)`)\n",
    "- Print the trickiest missclassified objects. Why they were hard to classify? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import time as tm\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.metrics import f1_score, precision_recall_curve, roc_curve, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lead time: 879.605 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_model = RandomForestClassifier(n_estimators=500)\n",
    "\n",
    "start = time()\n",
    "rf_model.fit(X_train_0, y_train)\n",
    "end = time()\n",
    "print('lead time: {:.3f} seconds'.format(end - start))\n",
    "\n",
    "rf_pred_train = rf_model.predict(X_train_0)\n",
    "rf_pred_test = rf_model.predict(X_test_0)\n",
    "\n",
    "accuracy_score_rf_train = accuracy_score(y_train, rf_pred_train)\n",
    "accuracy_score_rf_test = accuracy_score(y_test, rf_pred_test)\n",
    "\n",
    "precision_score_rf_train = precision_score(y_train, rf_pred_train)\n",
    "precision_score_rf_test = precision_score(y_test, rf_pred_test)\n",
    "\n",
    "recall_score_rf_train = recall_score(y_train, rf_pred_train)\n",
    "recall_score_rf_test = recall_score(y_test, rf_pred_test)\n",
    "\n",
    "f1_score_rf_train = f1_score(y_train, rf_pred_train)\n",
    "f1_score_rf_test  = f1_score(y_test, rf_pred_test)\n",
    "\n",
    "precision_recall_curve_rf_train = precision_recall_curve(y_train, rf_pred_train)\n",
    "precision_recall_curve_rf_test  = precision_recall_curve(y_test, rf_pred_test)\n",
    "\n",
    "roc_curve_rf_train = roc_curve(y_train, rf_pred_train)\n",
    "roc_curve_rf_test  = roc_curve(y_test, rf_pred_test)\n",
    "\n",
    "roc_auc_score_rf_train = roc_auc_score(y_train, rf_pred_train)\n",
    "roc_auc_score_rf_test  = roc_auc_score(y_test, rf_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lead time: 17.527 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr_model = LogisticRegression(max_iter=1e5)\n",
    "\n",
    "start = time()\n",
    "lr_model.fit(X_train_0, y_train)\n",
    "end = time()\n",
    "print('lead time: {:.3f} seconds'.format(end - start))\n",
    "\n",
    "lr_pred_train = lr_model.predict(X_train_0)\n",
    "lr_pred_test  = lr_model.predict(X_test_0)\n",
    "\n",
    "precision_score_lr_train = precision_score(y_train, lr_pred_train)\n",
    "precision_score_lr_test  = precision_score(y_test,  lr_pred_test)\n",
    "\n",
    "recall_score_lr_train = recall_score(y_train, lr_pred_train)\n",
    "recall_score_lr_test  = recall_score(y_test,  lr_pred_test)\n",
    "\n",
    "f1_score_lr_train = f1_score(y_train, lr_pred_train)\n",
    "f1_score_lr_test  = f1_score(y_test, lr_pred_test)\n",
    "\n",
    "precision_recall_curve_lr_train = precision_recall_curve(y_train, lr_pred_train)\n",
    "precision_recall_curve_lr_test  = precision_recall_curve(y_test, lr_pred_test)\n",
    "\n",
    "roc_curve_lr_train = roc_curve(y_train, lr_pred_train)\n",
    "roc_curve_lr_test  = roc_curve(y_test, lr_pred_test)\n",
    "\n",
    "roc_auc_score_lr_train = roc_auc_score(y_train, lr_pred_train)\n",
    "roc_auc_score_lr_test  = roc_auc_score(y_test, lr_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$%#d = {'model':  ['RandomForestClassifier', 'LogisticRegression'] ,     \n",
    "%#     'precision_score_train':   [precision_score_rf_train, precision_score_lr_train],\n",
    "%#     'precision_score_test':    [precision_score_rf_test,  precision_score_lr_test],\n",
    "%#     'recall_score_train':      [recall_score_rf_train,    recall_score_lr_train],\n",
    "%#     'recall_score_test':       [recall_score_rf_test,     recall_score_lr_test],\n",
    "%#     'f1_score_train':          [f1_score_rf_train,    f1_score_lr_train],\n",
    "%#     'f1_score_test':           [f1_score_rf_test,     f1_score_lr_test],\n",
    "%     #'precision_recall_curve_train':          [precision_recall_curve_rf_train,    %precision_recall_curve_lr_train],\n",
    "%     #'precision_recall_curve_test':           [precision_recall_curve_rf_test,     %precision_recall_curve_lr_test],\n",
    "%     #'roc_curve_train':                [roc_curve_rf_train,    roc_curve_lr_train],\n",
    "%     #'roc_curve_test':                [roc_curve_rf_test,      roc_curve_lr_test],\n",
    "%#     'roc_auc_score_train':          [roc_auc_score_rf_train,    roc_auc_score_lr_train],\n",
    "%#     'roc_auc_score_test':           [roc_auc_score_rf_test,     roc_auc_score_lr_test]}\n",
    "%#pd.DataFrame(data=d)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <th>LogisticRegression</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>precision_score_train</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision_score_test</th>\n",
       "      <td>0.846273</td>\n",
       "      <td>0.875640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall_score_train</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall_score_test</th>\n",
       "      <td>0.872000</td>\n",
       "      <td>0.890000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1_score_train</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1_score_test</th>\n",
       "      <td>0.858944</td>\n",
       "      <td>0.882761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roc_auc_score_train</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roc_auc_score_test</th>\n",
       "      <td>0.856800</td>\n",
       "      <td>0.881800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       RandomForestClassifier  LogisticRegression\n",
       "precision_score_train                1.000000            0.999200\n",
       "precision_score_test                 0.846273            0.875640\n",
       "recall_score_train                   1.000000            0.998800\n",
       "recall_score_test                    0.872000            0.890000\n",
       "f1_score_train                       1.000000            0.999000\n",
       "f1_score_test                        0.858944            0.882761\n",
       "roc_auc_score_train                  1.000000            0.999000\n",
       "roc_auc_score_test                   0.856800            0.881800"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {'RandomForestClassifier':   [precision_score_rf_train, \n",
    "                                  precision_score_rf_test,\n",
    "                                     recall_score_rf_train,\n",
    "                                     recall_score_rf_test,\n",
    "                                         f1_score_rf_train,\n",
    "                                         f1_score_rf_test,\n",
    "                                    roc_auc_score_rf_train, \n",
    "                                    roc_auc_score_rf_test],\n",
    "     'LogisticRegression':       [precision_score_lr_train,  \n",
    "                                  precision_score_lr_test,\n",
    "                                     recall_score_lr_train,\n",
    "                                     recall_score_lr_test,\n",
    "                                         f1_score_lr_train,\n",
    "                                         f1_score_lr_test,\n",
    "                                    roc_auc_score_lr_train,\n",
    "                                    roc_auc_score_lr_test]}\n",
    "pd.DataFrame(data=d, index = ['precision_score_train',\n",
    "                              'precision_score_test',\n",
    "                              'recall_score_train',\n",
    "                              'recall_score_test',\n",
    "                              'f1_score_train',\n",
    "                              'f1_score_test',\n",
    "                              'roc_auc_score_train', \n",
    "                              'roc_auc_score_test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name several types how those special matrices are stored and what they are good for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which model gives higher scores? Any ideas why? Please suggest 1-2 reasons.\n",
    "\n",
    "*Answer:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ %RandomForestClassifier показал лучше результат. $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LogisticRegression показал лучше результат."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More sophisticated feature prerocessing\n",
    "\n",
    "As we have seen, simple BOW can give us some result - it's time to improve it.\n",
    "\n",
    "**Task 1.4 [1 point] - Frequencies calculation**\n",
    "\n",
    "- Calculate top-20 words in train set and test set. *Are they meaningful?*\n",
    "- Import `stopwords` and print some of them. What are those?\n",
    "- Recalculate top-20 words in each set, but exclude stop words.\n",
    "- Does now top-20 include more useful words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk.tokenize import WhitespaceTokenizer, WordPunctTokenizer, TreebankWordTokenizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pythonworld.ru/moduli/modul-collections.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate top-20 words in train set and test set. Are they meaningful?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15061    A very silly movie, this starts with a soft po...\n",
       "10112    1st watched 8/3/2003 - 2 out of 10(Dir-Brad Sy...\n",
       "24550    This is a really heart-warming family movie. I...\n",
       "2570     Nicole Kidman is a wonderful actress and here ...\n",
       "16053    It's very hard to say just what was going on w...\n",
       "                               ...                        \n",
       "20210    The Three Stooges has always been some of the ...\n",
       "15989    And a made for TV movie too, this movie was go...\n",
       "20873    Back in my days as an usher \\Private Lessons\\\"...\n",
       "10422    I might not be a huge admirer of the original ...\n",
       "21768    This movie was an impressive one. My first exp...\n",
       "Name: review, Length: 20000, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#top-20 words in train set\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = ' '.join(map(str, X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 230191),\n",
       " ('a', 124236),\n",
       " ('and', 122525),\n",
       " ('of', 114632),\n",
       " ('to', 106231),\n",
       " ('is', 82852),\n",
       " ('in', 68303),\n",
       " ('I', 52896),\n",
       " ('that', 51864),\n",
       " ('this', 45937),\n",
       " ('it', 43691),\n",
       " ('/><br', 40824),\n",
       " ('was', 37452),\n",
       " ('as', 33868),\n",
       " ('with', 33567),\n",
       " ('for', 32891),\n",
       " ('The', 27102),\n",
       " ('but', 27011),\n",
       " ('on', 24827),\n",
       " ('movie', 24625)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Whitespace = WhitespaceTokenizer().tokenize(s)\n",
    "Counter_Whitespace = Counter(Whitespace)\n",
    "#sorted(Counter_Whitespace.items())\n",
    "top = [(l,k) for k,l in sorted([(j,i) for i,j in Counter_Whitespace.items()], reverse=True)]\n",
    "top[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 232718),\n",
       " (',', 210946),\n",
       " ('.', 180270),\n",
       " ('and', 126065),\n",
       " ('a', 125408),\n",
       " ('of', 115592),\n",
       " ('to', 107393),\n",
       " (\"'\", 104320),\n",
       " ('is', 85332),\n",
       " ('br', 81648),\n",
       " ('in', 70114),\n",
       " ('I', 65898),\n",
       " ('it', 62493),\n",
       " ('that', 56535),\n",
       " ('s', 50464),\n",
       " ('this', 48839),\n",
       " ('-', 44759),\n",
       " ('/><', 40824),\n",
       " ('/>', 39157),\n",
       " ('was', 38383)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WordPunct = WordPunctTokenizer().tokenize(s)\n",
    "Counter_WordPunct = Counter(WordPunct)\n",
    "top = [(l,k) for k,l in sorted([(j,i) for i,j in Counter_WordPunct.items()], reverse=True)]\n",
    "top[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 231895),\n",
       " (',', 221238),\n",
       " ('and', 125458),\n",
       " ('a', 125014),\n",
       " ('of', 115057),\n",
       " ('to', 106660),\n",
       " ('is', 86915),\n",
       " ('/', 81829),\n",
       " ('>', 81789),\n",
       " ('<', 81742),\n",
       " ('br', 81648),\n",
       " ('in', 69149),\n",
       " ('I', 65008),\n",
       " ('it', 56833),\n",
       " ('that', 55557),\n",
       " (\"''\", 52892),\n",
       " (\"'s\", 49338),\n",
       " ('this', 47639),\n",
       " ('was', 39719),\n",
       " ('as', 34629)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TreebankWord = TreebankWordTokenizer().tokenize(s)\n",
    "Counter_TreebankWord = Counter(TreebankWord)\n",
    "top = [(l,k) for k,l in sorted([(j,i) for i,j in Counter_TreebankWord.items()], reverse=True)]\n",
    "top[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Слова не значимы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.5 [1 point] - Word Freqs by class**\n",
    "\n",
    "How do you think, will top100 tokens for positive and negative classes be different? Use data to prove your point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_class = X[y==1] \n",
    "negative_class = X[y==0] \n",
    "positive_class.shape[0] + negative_class.shape[0] == X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        With all this stuff going down at the moment w...\n",
       "1        \\The Classic War of the Worlds\\\" by Timothy Hi...\n",
       "4        Superbly trashy and wondrously unpretentious 8...\n",
       "5        I dont know why people think this is such a ba...\n",
       "9        <br /><br />This movie is full of references. ...\n",
       "                               ...                        \n",
       "24987    First off, I'd like to make a correction on an...\n",
       "24988    While originally reluctant to jump on the band...\n",
       "24989    I heard about this movie when watching VH1's \\...\n",
       "24990    I've never been huge on IMAX films. They're co...\n",
       "24999    I saw this movie as a child and it broke my he...\n",
       "Name: review, Length: 12500, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2        The film starts with a manager (Nicholas Bell)...\n",
       "3        It must be assumed that those who praised this...\n",
       "6        This movie could have been very good, but come...\n",
       "7        I watched this video at a friend's house. I'm ...\n",
       "8        A friend of mine bought this film for £1, and ...\n",
       "                               ...                        \n",
       "24994    Unimaginably stupid, redundant and humiliating...\n",
       "24995    It seems like more consideration has gone into...\n",
       "24996    I don't believe they made this film. Completel...\n",
       "24997    Guy is a loser. Can't get girls, needs to buil...\n",
       "24998    This 30 minute documentary Buñuel made in the ...\n",
       "Name: review, Length: 12500, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[24994]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_positive = ' '.join(map(str, positive_class))\n",
    "s_negative = ' '.join(map(str, negative_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Whitespace = WhitespaceTokenizer().tokenize(s_positive)\n",
    "Counter_Whitespace = Counter(Whitespace)\n",
    "top_1 = [(l,k) for k,l in sorted([(j,i) for i,j in Counter_Whitespace.items()], reverse=True)]\n",
    "#top_1[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 138618),\n",
       " ('a', 75668),\n",
       " ('and', 68388),\n",
       " ('of', 67631),\n",
       " ('to', 67359),\n",
       " ('is', 47871),\n",
       " ('in', 39784),\n",
       " ('I', 35045),\n",
       " ('that', 32617),\n",
       " ('this', 31174),\n",
       " ('it', 27443),\n",
       " ('/><br', 26318),\n",
       " ('was', 25389),\n",
       " ('for', 20199),\n",
       " ('with', 19689),\n",
       " ('as', 18580),\n",
       " ('but', 17331),\n",
       " ('movie', 17129),\n",
       " ('The', 17100),\n",
       " ('on', 15380)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Whitespace = WhitespaceTokenizer().tokenize(s_negative)\n",
    "Counter_Whitespace = Counter(Whitespace)\n",
    "top_2 = [(l,k) for k,l in sorted([(j,i) for i,j in Counter_Whitespace.items()], reverse=True)]\n",
    "#top_2[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer:* по большей части совпадают"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.6 [2 points] - Reducing dimensionality**\n",
    "\n",
    "The goal is to reduce number of features to 15000.\n",
    "\n",
    "Implement the following methods of dimensinality reduction:\n",
    "1. Use CountVectorizer, but leave only 15k most frequent tokens\n",
    "2. Use HashingVectorizer with 15k features\n",
    "3. Use 15k most important features from perspective of previously trained RandomForest\n",
    "\n",
    "*Hints:*\n",
    "- in 1 and 2 you don't have to apply nltk.corpus.stopwords, vectorizers have `stopwords` parameter\n",
    "- in 1 look for `vocabulary` parameter\n",
    "- in 3... remember `lab02`? You may use `X_train_0` and `X_test_0` as input matrices\n",
    "\n",
    "Train LogisticRegression and RandomForest on each dataset and compare ROC AUC scores of the classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Use CountVectorizer, but leave only 15k most frequent tokens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<20000x74849 sparse matrix of type '<class 'numpy.int64'>'\n",
       " \twith 2760558 stored elements in Compressed Sparse Row format>,\n",
       " <5000x74849 sparse matrix of type '<class 'numpy.int64'>'\n",
       " \twith 685303 stored elements in Compressed Sparse Row format>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_0, X_test_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        With all this stuff going down at the moment w...\n",
       "1        \\The Classic War of the Worlds\\\" by Timothy Hi...\n",
       "2        The film starts with a manager (Nicholas Bell)...\n",
       "3        It must be assumed that those who praised this...\n",
       "4        Superbly trashy and wondrously unpretentious 8...\n",
       "                               ...                        \n",
       "24995    It seems like more consideration has gone into...\n",
       "24996    I don't believe they made this film. Completel...\n",
       "24997    Guy is a loser. Can't get girls, needs to buil...\n",
       "24998    This 30 minute documentary Buñuel made in the ...\n",
       "24999    I saw this movie as a child and it broke my he...\n",
       "Name: review, Length: 25000, dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = ' '.join(map(str, X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#s = ' '.join(map(str, X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A very sil'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#TreebankWord = TreebankWordTokenizer().tokenize(s)\n",
    "#Counter_TreebankWord = Counter(TreebankWord)\n",
    "#top = [(l,k) for k,l in sorted([(j,i) for i,j in Counter_TreebankWord.items()], reverse=True)]\n",
    "#top[0:20]\n",
    "\n",
    "#MemoryError\n",
    "#----> 1 TreebankWord = TreebankWordTokenizer().tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Whitespace = WhitespaceTokenizer().tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Whitespace = WhitespaceTokenizer().tokenize(s)\n",
    "#Whitespace = WhitespaceTokenizer().tokenize(s)\n",
    "Counter_Whitespace = Counter(Whitespace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Whitespace = WhitespaceTokenizer().tokenize(s)\n",
    "vectorizer = CountVectorizer()\n",
    "#Counter_Whitespace = Counter(Whitespace)\n",
    "top = [(l,k) for k,l in sorted([(j,i) for i,j in Counter_Whitespace.items()], reverse=True)]\n",
    "top[15000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter_Whitespace = Counter(Whitespace, max_features = 15000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> from sklearn.feature_extraction.text import CountVectorizer\n",
    ">>> corpus = [\n",
    "...     'This is the first document.',\n",
    "...     'This document is the second document.',\n",
    "...     'And this is the third one.',\n",
    "...     'Is this the first document?',\n",
    "... ]\n",
    ">>> vectorizer = CountVectorizer()\n",
    ">>> X = vectorizer.fit_transform(corpus)\n",
    ">>> vectorizer.get_feature_names_out()\n",
    "array(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third',\n",
    "       'this'], ...)\n",
    ">>> print(X.toarray())\n",
    "[[0 1 1 1 0 0 1 0 1]\n",
    " [0 2 0 1 0 1 1 0 1]\n",
    " [1 0 0 1 1 0 1 1 1]\n",
    " [0 1 1 1 0 0 1 0 1]]\n",
    ">>> vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
    ">>> X2 = vectorizer2.fit_transform(corpus)\n",
    ">>> vectorizer2.get_feature_names_out()\n",
    "array(['and this', 'document is', 'first document', 'is the', 'is this',\n",
    "       'second document', 'the first', 'the second', 'the third', 'third one',\n",
    "       'this document', 'this is', 'this the'], ...)\n",
    " >>> print(X2.toarray())\n",
    " [[0 0 1 1 0 0 1 0 0 0 0 1 0]\n",
    " [0 1 0 1 0 1 0 1 0 0 1 0 0]\n",
    " [1 0 0 1 0 0 0 0 1 1 0 1 0]\n",
    " [0 0 1 0 1 0 1 0 0 0 0 0 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> from sklearn.feature_extraction.text import HashingVectorizer\n",
    ">>> corpus = [\n",
    "...     'This is the first document.',\n",
    "...     'This document is the second document.',\n",
    "...     'And this is the third one.',\n",
    "...     'Is this the first document?',\n",
    "... ]\n",
    ">>> vectorizer = HashingVectorizer(n_features=2**4)\n",
    ">>> X = vectorizer.fit_transform(corpus)\n",
    ">>> print(X.shape)\n",
    "(4, 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.7 [2 points] - Token Normalization**\n",
    "\n",
    "Choose the best working method from previous task. Try improve it by applying a token normalization technique.\n",
    "\n",
    "You may use one of normalizers imported below, but feel free to experiment.\n",
    "\n",
    "Do the following:\n",
    "- Apply normalizer to X_train, X_test\n",
    "- Build BOW with CountVectorizer + stopwords. What are the shapes of train and test matrices now?\n",
    "- Reduce dimensionality with the best method from Task 2.6. You may try all of them\n",
    "- Train LR/RF to examine whether ROC AUC or Accuracy was improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer, PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Word Embeddings [7 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-b8adb223ee22>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownloader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "import gensim.downloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the list of pretrained word embedding models. We suggest using `glove-wiki-gigaword-100`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(gensim.downloader.info()['models'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = gensim.downloader.load(\"glove-wiki-gigaword-100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.1 [2 point] - WordEmbeddings Geometry**\n",
    "\n",
    "As you probably know, vector space of word embeddings has non-trivial geometry: some word relations (like country-capital or single-plural) cab be represented by vectors, like: **(king - man) + woman = queen**\n",
    "\n",
    "<img src=\"https://linkme.ufanet.ru/images/5687a2011b49eb2413912f1c7d0fb0bd.png\" width=600px>\n",
    "\n",
    "Check this statement on words from the above picture with `word_embeddings.most_similar` function. Pay attention to `positive` and `negative` params.\n",
    "\n",
    "Provide **several** examples, make sure to present different relations: some for nouns, some for verbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.2 [2 point] - POS analysis**\n",
    "\n",
    "Use POS tagger to calculate most common POS in the dataset. \n",
    "Here you may read about nltk-taggers: [link](https://www.inf.ed.ac.uk/teaching/courses/icl/nltk/tagging.pdf)\n",
    "\n",
    "- If you were to design POS-related weights, how would you do it? \n",
    "- What POS would get the higher weight? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.3 [3 points] - WordEmbeddings**\n",
    "\n",
    "Use dense vector representations to construct vector-representation of each review, then train a model (LR or RF).\n",
    "\n",
    "Compare results of the new model to results of the models above.\n",
    "**Important**\n",
    "- If you just sum embeddings of each token to get an embedding of the whole review, the cost of the task is **[2 points]**\n",
    "- For **[3 points]** you have to use either TF-IDF weight or weights that you designed from POS tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
